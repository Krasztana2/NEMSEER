{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Price Convergence in 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will take a look at energy price convergence. \n",
    "\n",
    "To do this, we'll look at price forecasts from (30-minute) pre-dispatch ({term}`PREDISPATCH`) and 5-minute pre-dispatch ({term}`P5MIN`). \n",
    "\n",
    "We'll also look at the relationship between demand forecast error and energy price convergence.\n",
    "\n",
    "```{note}\n",
    "While we sometimes use *price error* to refer to the difference between actual prices and forecast prices in some parts of this example, we prefer *price convergence*. The intention of `P5MIN` and `PREDISPATCH` is to provide AEMO and participants with up-to-date system and market information. A potential outcome of this is that participants change their decisions (e.g. rebidding, as we note [here](https://github.com/UNSW-CEEM/NEMSEER#user-content-fn-1-8822583f56aa96a7caaf6d6d5e453504)). As such, prices reported in `P5MIN` and `PREDISPATCH` should not not strictly interpreted as forecasts.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# NEM data libraries\n",
    "# NEMOSIS for actual demand data\n",
    "# NEMSEER for forecast demand data\n",
    "import nemosis\n",
    "from nemseer import compile_data, download_raw_data, generate_runtimes\n",
    "\n",
    "# data wrangling libraries\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# interactive plotting\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# static plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# silence NEMSEER and NEMOSIS logging\n",
    "logging.getLogger(\"nemosis\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"nemseer\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our analysis start and end dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_start = \"2021/01/01 00:00:00\"\n",
    "analysis_end = \"2022/01/01 00:00:00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AEMO data tables, the energy price in \\$/MW/hr is usually found in the `RRP` column.\n",
    "\n",
    "We will focus on 5-minute dispatch interval prices.\n",
    "\n",
    "```{note}\n",
    "Prior to midnight 30 September (commencement of 5-minute settlement), market settlement prices for energy were calculated as the average of the 6 dispatch interval prices in a half hour window.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining actual price data from `NEMOSIS`\n",
    "\n",
    "We will download `DISPATCHPRICE` to access the `RRP` (energy price) field and cache it so that it's ready for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemosis_cache = Path(\"nemosis_cache/\")\n",
    "if not nemosis_cache.exists():\n",
    "    nemosis_cache.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemosis.cache_compiler(\n",
    "    analysis_start, analysis_end, \"DISPATCHPRICE\", nemosis_cache, fformat=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining actual demand data from `NEMOSIS`\n",
    "\n",
    "We can download `DISPATCHREGIONSUM` to get actual demand values used in dispatch (`TOTALDEMAND`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemosis.cache_compiler(\n",
    "    analysis_start, analysis_end, \"DISPATCHREGIONSUM\", nemosis_cache, fformat=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining forecast price data from `NEMSEER`\n",
    "\n",
    "We will download `PRICE` to access the `RRP` field in `PREDISPATCH` forecasts, and `REGIONSOLUTION` to access the `RRP` field in `P5MIN` forecasts. We'll cache it so that it's ready for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_raw_data(\n",
    "    \"PREDISPATCH\",\n",
    "    \"PRICE\",\n",
    "    \"nemseer_cache/\",\n",
    "    forecasted_start=analysis_start,\n",
    "    forecasted_end=analysis_end,\n",
    ")\n",
    "\n",
    "\n",
    "download_raw_data(\n",
    "    \"P5MIN\",\n",
    "    \"REGIONSOLUTION\",\n",
    "    \"nemseer_cache/\",\n",
    "    forecasted_start=analysis_start,\n",
    "    forecasted_end=analysis_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining forecast demand data from NEMSEER\n",
    "\n",
    "We will also download demand data. This is contained within `REGIONSOLUTION` for `P5MIN`, but we need to get the table `REGIONSUM` for `PREDISPATCH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_raw_data(\n",
    "    \"PREDISPATCH\",\n",
    "    \"REGIONSUM\",\n",
    "    \"nemseer_cache/\",\n",
    "    forecasted_start=analysis_start,\n",
    "    forecasted_end=analysis_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing price (and demand) convergence for a particular time\n",
    "\n",
    "The code below (toggle to unhide) allows us to plot forecast convergence for price and demand across `PREDISPATCH` and `P5MIN`.\n",
    "\n",
    "Note that we adjust the *nominal* {term}`run time` to the {term}`actual run time` for both forecast types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_price_comparison(time: str, regionid: str) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Creates a figure that compares price forecasts from PD & P5MIN against\n",
    "    the actual price.\n",
    "\n",
    "    Args:\n",
    "        time: Datetime string in format YYYY/mm/dd HH:MM:SS\n",
    "        regionid: One of (\"NSW1\", \"QLD1\", \"VIC1\", \"TAS1\", \"SA1\")\n",
    "    Returns:\n",
    "        plotly Figure\n",
    "    \"\"\"\n",
    "\n",
    "    def get_actual_data(time: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Gets actual price data\n",
    "        \"\"\"\n",
    "        # get actual data from the hour beforehand to the interval of interest\n",
    "        nemosis_window = (\n",
    "            (\n",
    "                datetime.strptime(time, \"%Y/%m/%d %H:%M:%S\") - timedelta(minutes=60)\n",
    "            ).strftime(\"%Y/%m/%d %H:%M:%S\"),\n",
    "            time,\n",
    "        )\n",
    "        nemosis_price = nemosis.dynamic_data_compiler(\n",
    "            nemosis_window[0],\n",
    "            nemosis_window[1],\n",
    "            \"DISPATCHPRICE\",\n",
    "            nemosis_cache,\n",
    "            filter_cols=[\"INTERVENTION\"],\n",
    "            filter_values=([0],),\n",
    "        )\n",
    "        actual_price = nemosis_price[[\"SETTLEMENTDATE\", \"REGIONID\", \"RRP\"]]\n",
    "        return actual_price\n",
    "\n",
    "    def get_forecast_data(time: str) -> Tuple[xr.DataArray, xr.DataArray]:\n",
    "        \"\"\"\n",
    "        Gets P5 and PD forecast price data\n",
    "        Also corrects nominal to actual run time\n",
    "        \"\"\"\n",
    "        # get P5 and PD forecast data\n",
    "        ## get PD data\n",
    "        run_start, run_end = generate_runtimes(time, time, \"PREDISPATCH\")\n",
    "        pd_price = compile_data(\n",
    "            run_start,\n",
    "            run_end,\n",
    "            time,\n",
    "            time,\n",
    "            \"PREDISPATCH\",\n",
    "            \"PRICE\",\n",
    "            \"nemseer_cache/\",\n",
    "            data_format=\"xr\",\n",
    "        )[\"PRICE\"][\"RRP\"]\n",
    "        ## calculate actual run time from run time, then swap out nominal for actual\n",
    "        pd_price = pd_price.assign_coords(\n",
    "            {\"actual_run_time\": pd_price.coords[\"run_time\"] - pd.Timedelta(30, \"T\")}\n",
    "        )\n",
    "        pd_price = pd_price.swap_dims({\"run_time\": \"actual_run_time\"}).drop(\"run_time\")\n",
    "        ## get P5 data\n",
    "        p5_price = compile_data(\n",
    "            run_start,\n",
    "            run_end,\n",
    "            time,\n",
    "            time,\n",
    "            \"P5MIN\",\n",
    "            \"REGIONSOLUTION\",\n",
    "            \"nemseer_cache/\",\n",
    "            data_format=\"xr\",\n",
    "        )[\"REGIONSOLUTION\"][\"RRP\"]\n",
    "        ## calculate actual run time from run time, then swap out nominal for actual\n",
    "        p5_price = p5_price.assign_coords(\n",
    "            {\"actual_run_time\": p5_price.coords[\"run_time\"] - pd.Timedelta(5, \"T\")}\n",
    "        )\n",
    "        p5_price = p5_price.swap_dims({\"run_time\": \"actual_run_time\"}).drop(\"run_time\")\n",
    "        return pd_price, p5_price\n",
    "\n",
    "    actual_price = get_actual_data(time).query(\"REGIONID==@regionid\")\n",
    "    pd_price, p5_price = get_forecast_data(time)\n",
    "    pd_price = pd_price.sel(REGIONID=regionid)\n",
    "    p5_price = p5_price.sel(REGIONID=regionid)\n",
    "    # create plotly figure\n",
    "    fig = go.Figure(\n",
    "        layout=dict(\n",
    "            title=f\"Energy Price Comparison: {regionid} {time} <br><sup>Actual run times for forecasts</sup>\"\n",
    "        )\n",
    "    )\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=pd_price.isel(forecasted_time=0).to_dataframe().index,\n",
    "                y=pd_price.isel(forecasted_time=0).to_dataframe()[\"RRP\"],\n",
    "                name=\"PD\",\n",
    "                mode=\"lines\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=p5_price.isel(forecasted_time=0).to_dataframe().index,\n",
    "                y=p5_price.isel(forecasted_time=0).to_dataframe()[\"RRP\"],\n",
    "                name=\"P5\",\n",
    "                mode=\"lines\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=actual_price[\"SETTLEMENTDATE\"],\n",
    "                y=actual_price[\"RRP\"],\n",
    "                name=\"actual\",\n",
    "                mode=\"lines\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_demand_comparison(time: str, regionid: str) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Creates a figure that compares demand forecasts from PD & P5MIN against\n",
    "    the actual demand used in dispatch.\n",
    "\n",
    "    Args:\n",
    "        time: Datetime string in format YYYY/mm/dd HH:MM:SS\n",
    "        regionid: One of (\"NSW1\", \"QLD1\", \"VIC1\", \"TAS1\", \"SA1\")\n",
    "    Returns:\n",
    "        plotly Figure\n",
    "    \"\"\"\n",
    "\n",
    "    def get_actual_data(time: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Gets actual demand data\n",
    "        \"\"\"\n",
    "        # get actual data from the hour beforehand to the interval of interest\n",
    "        nemosis_window = (\n",
    "            (\n",
    "                datetime.strptime(time, \"%Y/%m/%d %H:%M:%S\") - timedelta(minutes=60)\n",
    "            ).strftime(\"%Y/%m/%d %H:%M:%S\"),\n",
    "            time,\n",
    "        )\n",
    "        nemosis_demand = nemosis.dynamic_data_compiler(\n",
    "            nemosis_window[0],\n",
    "            nemosis_window[1],\n",
    "            \"DISPATCHREGIONSUM\",\n",
    "            nemosis_cache,\n",
    "            filter_cols=[\"INTERVENTION\"],\n",
    "            filter_values=([0],),\n",
    "        )\n",
    "        actual_demand = nemosis_demand[[\"SETTLEMENTDATE\", \"REGIONID\", \"TOTALDEMAND\"]]\n",
    "        return actual_demand.sort_values(\"SETTLEMENTDATE\")\n",
    "\n",
    "    def get_forecast_data(time: str) -> Tuple[xr.DataArray, xr.DataArray]:\n",
    "        \"\"\"\n",
    "        Gets P5 and PD forecast demand data\n",
    "        Also corrects nominal to actual run time\n",
    "        \"\"\"\n",
    "        # get P5 and PD forecast data\n",
    "        ## get PD data\n",
    "        run_start, run_end = generate_runtimes(time, time, \"PREDISPATCH\")\n",
    "        pd_demand = compile_data(\n",
    "            run_start,\n",
    "            run_end,\n",
    "            time,\n",
    "            time,\n",
    "            \"PREDISPATCH\",\n",
    "            \"REGIONSUM\",\n",
    "            \"nemseer_cache/\",\n",
    "            data_format=\"xr\",\n",
    "        )[\"REGIONSUM\"][\"TOTALDEMAND\"]\n",
    "        ## calculate actual run time from run time, then swap out nominal for actual\n",
    "        pd_demand = pd_demand.assign_coords(\n",
    "            {\"actual_run_time\": pd_demand.coords[\"run_time\"] - pd.Timedelta(30, \"T\")}\n",
    "        )\n",
    "        pd_demand = pd_demand.swap_dims({\"run_time\": \"actual_run_time\"}).drop(\n",
    "            \"run_time\"\n",
    "        )\n",
    "        ## get P5 data\n",
    "        p5_demand = compile_data(\n",
    "            run_start,\n",
    "            run_end,\n",
    "            time,\n",
    "            time,\n",
    "            \"P5MIN\",\n",
    "            \"REGIONSOLUTION\",\n",
    "            \"nemseer_cache/\",\n",
    "            data_format=\"xr\",\n",
    "        )[\"REGIONSOLUTION\"][\"TOTALDEMAND\"]\n",
    "        ## calculate actual run time from run time, then swap out nominal for actual\n",
    "        p5_demand = p5_demand.assign_coords(\n",
    "            {\"actual_run_time\": p5_demand.coords[\"run_time\"] - pd.Timedelta(5, \"T\")}\n",
    "        )\n",
    "        p5_demand = p5_demand.swap_dims({\"run_time\": \"actual_run_time\"}).drop(\n",
    "            \"run_time\"\n",
    "        )\n",
    "        return pd_demand, p5_demand\n",
    "\n",
    "    actual_demand = get_actual_data(time).query(\"REGIONID==@regionid\")\n",
    "    pd_demand, p5_demand = get_forecast_data(time)\n",
    "    pd_demand = pd_demand.sel(REGIONID=regionid)\n",
    "    p5_demand = p5_demand.sel(REGIONID=regionid)\n",
    "    # create plotly figure\n",
    "    fig = go.Figure(\n",
    "        layout=dict(\n",
    "            title=f\"Demand Comparison: {regionid} {time} <br><sup>Actual run times for forecasts</sup>\"\n",
    "        )\n",
    "    )\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=pd_demand.isel(forecasted_time=0).to_dataframe().index,\n",
    "                y=pd_demand.isel(forecasted_time=0).to_dataframe()[\"TOTALDEMAND\"],\n",
    "                name=\"PD\",\n",
    "                mode=\"lines\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=p5_demand.isel(forecasted_time=0).to_dataframe().index,\n",
    "                y=p5_demand.isel(forecasted_time=0).to_dataframe()[\"TOTALDEMAND\"],\n",
    "                name=\"P5\",\n",
    "                mode=\"lines\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=actual_demand[\"SETTLEMENTDATE\"],\n",
    "                y=actual_demand[\"TOTALDEMAND\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"actual\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Plotting price and demand convergence on a summer's evening\n",
    "\n",
    "Let's plot an example - a summer's evening ramp event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = \"2021/12/30 18:00:00\"\n",
    "price_comp = plot_price_comparison(time, \"NSW1\")\n",
    "demand_comp = plot_demand_comparison(time, \"NSW1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pio.write_html(price_comp, \"../_static/price_cov.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "---\n",
    "file: ../_static/price_cov.html\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pio.write_html(demand_comp, \"../_static/demand_cov.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "---\n",
    "file: ../_static/demand_cov.html\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at price convergence over the year\n",
    "\n",
    "To try and look at convergence a bit more systematically, we'll compute the *\"price error\"* across 2021. \n",
    "\n",
    "The code below obtains `P5MIN` and `PREDISPATCH` price forecasts, removes overlapping forecasted periods (the last two `PREDISPATCH` forecasts overlap with `P5MIN` - these are removed from `PREDISPATCH`) and calculates a *\"price error\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_price_error(analysis_start: str, analysis_end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates price error in PREDISPATCH and P5MIN forecasts for periods between\n",
    "    analysis_start and analysis_end.\n",
    "\n",
    "    Args:\n",
    "        analysis_start: Start datetime, YYYY/mm/dd HH:MM:SS\n",
    "        analysis_end: End datetime, YYYY/mm/dd HH:MM:SS\n",
    "    Returns:\n",
    "        DataFrame with computed price error mapped to the ahead time of the\n",
    "        forecast and the forecasted time.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_actual_price_data() -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Gets actual price data\n",
    "        \"\"\"\n",
    "        # get actual demand data for forecasted_time\n",
    "        # nemosis start time must precede end of interval of interest by 5 minutes\n",
    "        nemosis_window = (\n",
    "            (\n",
    "                datetime.strptime(analysis_start, \"%Y/%m/%d %H:%M:%S\")\n",
    "                - timedelta(minutes=5)\n",
    "            ).strftime(\"%Y/%m/%d %H:%M:%S\"),\n",
    "            analysis_end,\n",
    "        )\n",
    "        nemosis_price = nemosis.dynamic_data_compiler(\n",
    "            nemosis_window[0],\n",
    "            nemosis_window[1],\n",
    "            \"DISPATCHPRICE\",\n",
    "            nemosis_cache,\n",
    "            filter_cols=[\"INTERVENTION\"],\n",
    "            filter_values=([0],),\n",
    "        )\n",
    "        actual_price = nemosis_price[[\"SETTLEMENTDATE\", \"REGIONID\", \"RRP\"]]\n",
    "        actual_price = actual_price.rename(\n",
    "            columns={\"SETTLEMENTDATE\": \"forecasted_time\"}\n",
    "        )\n",
    "        return actual_price\n",
    "\n",
    "    def get_forecast_price_data(ftype: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get price forecast data for the analysis period given a particular forecast type\n",
    "\n",
    "        Args:\n",
    "            ftype: 'P5MIN' or 'PREDISPATCH'\n",
    "        Returns:\n",
    "            DataFrame with price forecast data\n",
    "        \"\"\"\n",
    "        # ftype mappings\n",
    "        table = {\"PREDISPATCH\": \"PRICE\", \"P5MIN\": \"REGIONSOLUTION\"}\n",
    "        run_col = {\"PREDISPATCH\": \"PREDISPATCH_RUN_DATETIME\", \"P5MIN\": \"RUN_DATETIME\"}\n",
    "        forecasted_col = {\"PREDISPATCH\": \"DATETIME\", \"P5MIN\": \"INTERVAL_DATETIME\"}\n",
    "        # get run times\n",
    "        forecasts_run_start, forecasts_run_end = generate_runtimes(\n",
    "            analysis_start, analysis_end, ftype\n",
    "        )\n",
    "        df = compile_data(\n",
    "            forecasts_run_start,\n",
    "            forecasts_run_end,\n",
    "            analysis_start,\n",
    "            analysis_end,\n",
    "            ftype,\n",
    "            table[ftype],\n",
    "            \"nemseer_cache/\",\n",
    "        )[table[ftype]]\n",
    "        # remove intervention periods\n",
    "        df = df.query(\"INTERVENTION == 0\")\n",
    "        # rename run and forecasted time cols\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                run_col[ftype]: \"run_time\",\n",
    "                forecasted_col[ftype]: \"forecasted_time\",\n",
    "            }\n",
    "        )\n",
    "        # ensure values are sorted by forecasted and run times for nth groupby operation\n",
    "        return df[[\"run_time\", \"forecasted_time\", \"REGIONID\", \"RRP\"]].sort_values(\n",
    "            [\"forecasted_time\", \"run_time\"]\n",
    "        )\n",
    "\n",
    "    def combine_pd_p5_forecasts(\n",
    "        p5_df: pd.DataFrame, pd_df: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Combines P5 and PD forecasts, including removing PD overlap with P5\n",
    "        \"\"\"\n",
    "        # remove PD overlap with P5MIN\n",
    "        pd_nooverlap = pd_df.groupby(\n",
    "            [\"forecasted_time\", \"REGIONID\"], as_index=False\n",
    "        ).nth(slice(None, -2))\n",
    "        # concatenate and rename RRP to reflect that these are forecasted values\n",
    "        forecast_prices = pd.concat([pd_nooverlap, p5_df], axis=0).sort_values(\n",
    "            [\"forecasted_time\", \"actual_run_time\"]\n",
    "        )\n",
    "        forecast_prices = forecast_prices.rename(columns={\"RRP\": \"FORECASTED_RRP\"})\n",
    "        return forecast_prices\n",
    "\n",
    "    def process_price_error(\n",
    "        forecast_prices: pd.DataFrame, actual_price: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Merges actual and forecast prices and calculates ahead time and price error\n",
    "        \"\"\"\n",
    "        # left merge to ensure each forecasted price is mapped to its corresponding actual price\n",
    "        all_prices = pd.merge(\n",
    "            forecast_prices,\n",
    "            actual_price,\n",
    "            how=\"left\",\n",
    "            on=[\"forecasted_time\", \"REGIONID\"],\n",
    "        )\n",
    "        all_prices[\"ahead_time\"] = (\n",
    "            all_prices[\"forecasted_time\"] - all_prices[\"actual_run_time\"]\n",
    "        )\n",
    "        all_prices[\"error\"] = all_prices[\"RRP\"] - all_prices[\"FORECASTED_RRP\"]\n",
    "        price_error = all_prices.drop(\n",
    "            columns=[\"RRP\", \"FORECASTED_RRP\", \"actual_run_time\"]\n",
    "        )\n",
    "        return price_error\n",
    "\n",
    "    p5_df = get_forecast_price_data(\"P5MIN\")\n",
    "    pd_df = get_forecast_price_data(\"PREDISPATCH\")\n",
    "    # calulate actual run time for each forecast type\n",
    "    p5_df[\"actual_run_time\"] = p5_df[\"run_time\"] - pd.Timedelta(minutes=5)\n",
    "    pd_df[\"actual_run_time\"] = pd_df[\"run_time\"] - pd.Timedelta(minutes=30)\n",
    "    p5_df = p5_df.drop(columns=\"run_time\")\n",
    "    pd_df = pd_df.drop(columns=\"run_time\")\n",
    "    # get forecast prices\n",
    "    forecast_prices = combine_pd_p5_forecasts(p5_df, pd_df)\n",
    "\n",
    "    # actual prices\n",
    "    actual_price = get_actual_price_data()\n",
    "\n",
    "    price_error = process_price_error(forecast_prices, actual_price)\n",
    "    return price_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_error = calculate_price_error(analysis_start, analysis_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute price error for each region, by dispatch interval and ahead time\n",
    "\n",
    "Below we plot absolute price error (absolute value of price error) for each region across the year for forecasts:\n",
    "\n",
    "- 5 minutes ahead\n",
    "- 1 hour ahead\n",
    "- 5 hours ahead\n",
    "- 24 hours ahead\n",
    "\n",
    "Since forecasts 1+ hour ahead are only produced by `PREDISPATCH` and because `PREDISPATCH` is only run every half hour, we will only plot price errors for these half-hourly intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract ahead times of interest for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = (\"QLD1\", \"NSW1\", \"VIC1\", \"TAS1\", \"SA1\")\n",
    "aheads = [dict(minutes=5), dict(hours=1), dict(hours=5), dict(days=1)]\n",
    "region_errors = []\n",
    "for region in regions:\n",
    "    region_df = price_error.query(\"REGIONID==@region\").set_index(\"forecasted_time\")\n",
    "    # only keep select ahead times\n",
    "    ahead_df = region_df[region_df[\"ahead_time\"].isin(aheads)]\n",
    "    ahead_errors = []\n",
    "    # manual pivot to bring data for each ahead time into columns\n",
    "    for ahead in reversed(aheads):\n",
    "        time_unit = list(ahead.keys())[0]\n",
    "        ahead_time = list(ahead.values())[0]\n",
    "        desc = f\"{ahead_time} {time_unit} ahead\"\n",
    "        ahead_errors.append(\n",
    "            region_df[region_df[\"ahead_time\"] == timedelta(**ahead)][\"error\"].rename(\n",
    "                desc\n",
    "            )\n",
    "        )\n",
    "    # create a new DataFrame from ahead time series\n",
    "    # drop any rows with NAs. These will be intervals not on the half hour\n",
    "    ahead_df = pd.concat(ahead_errors, axis=1).dropna(axis=0)\n",
    "    region_errors.append(ahead_df.abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_price_error_figs = []\n",
    "for ahead_df, region in zip(region_errors, regions):\n",
    "    fig = px.line(\n",
    "        ahead_df,\n",
    "        title=(\n",
    "            f\"{region} Absolute Price Error \"\n",
    "            + \"<br><sup>Half-hourly dispatch intervals only</sup>\"\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(yaxis_title=\"Price Error\")\n",
    "    abs_price_error_figs.append(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "for region, fig in zip(regions, abs_price_error_figs):\n",
    "    pio.write_html(fig, f\"../_static/{region}_abs_price_error_2021.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "---\n",
    "file: ../_static/QLD1_abs_price_error_2021.html\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "---\n",
    "file: ../_static/NSW1_abs_price_error_2021.html\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "---\n",
    "file: ../_static/VIC1_abs_price_error_2021.html\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "---\n",
    "file: ../_static/TAS1_abs_price_error_2021.html\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "---\n",
    "file: ../_static/SA1_abs_price_error_2021.html\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_predispatch_demand_forecast_error_vectorised(\n",
    "    analysis_start: str, analysis_end: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates PD demand forecast error (Actual - Forecast) for all forecasts\n",
    "    that are run for a given forecasted_time in a vectorised fashion.\n",
    "\n",
    "    Args:\n",
    "        forecasted_time: Datetime string in the form YYYY/mm/dd HH:MM:SS\n",
    "    Returns:\n",
    "        pandas DataFrame with forecast error in `TOTALDEMAND` columns, the ahead time\n",
    "        of the forecast run in `ahead_time`, and the forecasted time in\n",
    "        `forecasted_time`.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_forecast_data(analysis_start: str, analysis_end: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use NEMSEER to get PD forecast data. Also omits any intervention periods.\n",
    "        \"\"\"\n",
    "        # use NEMSEER functions to compile pre-cached data\n",
    "        forecasts_run_start, forecasts_run_end = generate_runtimes(\n",
    "            analysis_start, analysis_end, \"PREDISPATCH\"\n",
    "        )\n",
    "        forecast_df = compile_data(\n",
    "            forecasts_run_start,\n",
    "            forecasts_run_end,\n",
    "            analysis_start,\n",
    "            analysis_end,\n",
    "            \"PREDISPATCH\",\n",
    "            \"REGIONSUM\",\n",
    "            \"nemseer_cache/\",\n",
    "        )[\"REGIONSUM\"]\n",
    "        # remove intervention periods\n",
    "        forecast_df = forecast_df.query(\"INTERVENTION == 0\")\n",
    "        return forecast_df\n",
    "\n",
    "    def get_actual_data(analysis_start: str, analysis_end: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use NEMOSIS to get actual data. Also omits any intervention periods\n",
    "        \"\"\"\n",
    "        # NEMOSIS start time must precede end of interval of interest by 5 minutes\n",
    "        nemosis_start = (\n",
    "            datetime.strptime(analysis_start, \"%Y/%m/%d %H:%M:%S\")\n",
    "            - timedelta(minutes=5)\n",
    "        ).strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "        # use NEMOSIS to compile pre-cached data and filter out interventions\n",
    "        actual_df = nemosis.dynamic_data_compiler(\n",
    "            nemosis_start,\n",
    "            analysis_end,\n",
    "            \"DISPATCHREGIONSUM\",\n",
    "            nemosis_cache,\n",
    "            filter_cols=[\"INTERVENTION\"],\n",
    "            filter_values=([0],),\n",
    "            fformat=\"parquet\",\n",
    "        )\n",
    "        return actual_df\n",
    "\n",
    "    def calculate_pd_forecast_demand_error(\n",
    "        actual_demand: pd.DataFrame, forecast_demand: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate PD forecast demand error given actual and forecast demand\n",
    "\n",
    "        Ahead time calculation reflects the fact that PD actual run time is\n",
    "        30 minutes before the nominal run time.\n",
    "        \"\"\"\n",
    "        # merge the two types of demand\n",
    "        merged = pd.merge(\n",
    "            forecast_demand,\n",
    "            actual_demand,\n",
    "            on=[\"forecasted_time\", \"REGIONID\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        if len(merged) > len(forecast_demand):\n",
    "            raise ValueError(\n",
    "                \"Merge should return DataFrame with dimensions of forecast data\"\n",
    "            )\n",
    "        # subtract 30 minutes from run time to get actual run time\n",
    "        merged[\"ahead_time\"] = merged[\"forecasted_time\"] - (\n",
    "            merged[\"run_time\"] - timedelta(minutes=30)\n",
    "        )\n",
    "        # calculate forecast error\n",
    "        forecast_error = (\n",
    "            merged[\"TOTALDEMAND\"] - merged[\"FORECAST_TOTALDEMAND\"]\n",
    "        ).rename(\"TOTALDEMAND\")\n",
    "        # create the forecast error DataFrame\n",
    "        forecast_error = pd.concat(\n",
    "            [forecast_error, merged[\"ahead_time\"], merged[\"REGIONID\"]], axis=1\n",
    "        ).set_index(merged[\"forecasted_time\"])\n",
    "        return forecast_error\n",
    "\n",
    "    # get forecast data\n",
    "    forecast_df = get_forecast_data(analysis_start, analysis_end)\n",
    "    # rename columns in preparation for merge\n",
    "    forecast_df = forecast_df.rename(\n",
    "        columns={\n",
    "            \"TOTALDEMAND\": \"FORECAST_TOTALDEMAND\",\n",
    "            \"DATETIME\": \"forecasted_time\",\n",
    "            \"PREDISPATCH_RUN_DATETIME\": \"run_time\",\n",
    "        }\n",
    "    )\n",
    "    forecast_demand = forecast_df[\n",
    "        [\"run_time\", \"forecasted_time\", \"REGIONID\", \"FORECAST_TOTALDEMAND\"]\n",
    "    ]\n",
    "\n",
    "    # get actual data\n",
    "    actual_df = get_actual_data(analysis_start, analysis_end)\n",
    "    # rename columns in preparation for merge\n",
    "    actual_df = actual_df.rename(\n",
    "        columns={\n",
    "            \"SETTLEMENTDATE\": \"forecasted_time\",\n",
    "            \"TOTALDEMAND\": \"TOTALDEMAND\",\n",
    "        }\n",
    "    )\n",
    "    actual_demand = actual_df[[\"forecasted_time\", \"REGIONID\", \"TOTALDEMAND\"]]\n",
    "\n",
    "    forecast_error = calculate_pd_forecast_demand_error(actual_demand, forecast_demand)\n",
    "    return forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_p5min_demand_forecast_error_vectorised(\n",
    "    analysis_start: str, analysis_end: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates P5MIN demand forecast error (Actual - Forecast) for all forecasts\n",
    "    that are run for a given forecasted_time in a vectorised fashion.\n",
    "\n",
    "    Args:\n",
    "        forecasted_time: Datetime string in the form YYYY/mm/dd HH:MM:SS\n",
    "    Returns:\n",
    "        pandas DataFrame with forecast error in `TOTALDEMAND` columns, the ahead time\n",
    "        of the forecast run in `ahead_time`, and the forecasted time in\n",
    "        `forecasted_time`.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_forecast_data(analysis_start: str, analysis_end: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use NEMSEER to get 5MPD forecast data. Also omits any intervention periods.\n",
    "        \"\"\"\n",
    "        # use NEMSEER functions to compile pre-cached data\n",
    "        forecasts_run_start, forecasts_run_end = generate_runtimes(\n",
    "            analysis_start, analysis_end, \"P5MIN\"\n",
    "        )\n",
    "        forecast_df = compile_data(\n",
    "            forecasts_run_start,\n",
    "            forecasts_run_end,\n",
    "            analysis_start,\n",
    "            analysis_end,\n",
    "            \"P5MIN\",\n",
    "            \"REGIONSOLUTION\",\n",
    "            \"nemseer_cache/\",\n",
    "        )[\"REGIONSOLUTION\"]\n",
    "        # remove intervention periods\n",
    "        forecast_df = forecast_df.query(\"INTERVENTION == 0\")\n",
    "        return forecast_df\n",
    "\n",
    "    def get_actual_data(analysis_start: str, analysis_end: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use NEMOSIS to get actual data. Also omits any intervention periods\n",
    "        \"\"\"\n",
    "        # NEMOSIS start time must precede end of interval of interest by 5 minutes\n",
    "        nemosis_start = (\n",
    "            datetime.strptime(analysis_start, \"%Y/%m/%d %H:%M:%S\")\n",
    "            - timedelta(minutes=5)\n",
    "        ).strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "        # use NEMOSIS to compile pre-cached data and filter out interventions\n",
    "        actual_df = nemosis.dynamic_data_compiler(\n",
    "            nemosis_start,\n",
    "            analysis_end,\n",
    "            \"DISPATCHREGIONSUM\",\n",
    "            nemosis_cache,\n",
    "            filter_cols=[\"INTERVENTION\"],\n",
    "            filter_values=([0],),\n",
    "            fformat=\"parquet\",\n",
    "        )\n",
    "        return actual_df\n",
    "\n",
    "    def calculate_p5min_forecast_demand_error(\n",
    "        actual_demand: pd.DataFrame, forecast_demand: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate P5MIN forecast demand error given actual and forecast demand\n",
    "\n",
    "        Ahead time calculation reflects the fact that P5MIN actual run time is\n",
    "        5 minutes before the nominal run time.\n",
    "        \"\"\"\n",
    "        # left merge ensures all forecasted values have the corresponding actual value merged in\n",
    "        merged = pd.merge(\n",
    "            forecast_demand, actual_demand, on=\"forecasted_time\", how=\"left\"\n",
    "        )\n",
    "        if len(merged) > len(forecast_demand):\n",
    "            raise ValueError(\n",
    "                \"Merge should return DataFrame with dimensions of forecast data\"\n",
    "            )\n",
    "        # subtract 5 minutes from run time to get actual run time\n",
    "        merged[\"ahead_time\"] = merged[\"forecasted_time\"] - (\n",
    "            merged[\"RUN_DATETIME\"] - timedelta(minutes=5)\n",
    "        )\n",
    "        forecast_error = (\n",
    "            merged[\"TOTALDEMAND\"] - merged[\"FORECAST_TOTALDEMAND\"]\n",
    "        ).rename(\"TOTALDEMAND\")\n",
    "        # create the forecast error DataFrame\n",
    "        forecast_error = pd.concat(\n",
    "            [forecast_error, merged[\"ahead_time\"]], axis=1\n",
    "        ).set_index(merged[\"forecasted_time\"])\n",
    "        return forecast_error\n",
    "\n",
    "    # get forecast data\n",
    "    forecast_df = get_forecast_data(analysis_start, analysis_end)\n",
    "    # rename columns in preparation for merge\n",
    "    forecast_df = forecast_df.rename(\n",
    "        columns={\n",
    "            \"TOTALDEMAND\": \"FORECAST_TOTALDEMAND\",\n",
    "            \"INTERVAL_DATETIME\": \"forecasted_time\",\n",
    "        }\n",
    "    )\n",
    "    # group by forecasted and run times, then sum demand across regions to get NEM-wide demand\n",
    "    forecast_demand = forecast_df.groupby([\"forecasted_time\", \"RUN_DATETIME\"])[\n",
    "        \"FORECAST_TOTALDEMAND\"\n",
    "    ].sum()\n",
    "    forecast_demand = forecast_demand.reset_index()\n",
    "\n",
    "    # get actual data\n",
    "    actual_df = get_actual_data(analysis_start, analysis_end)\n",
    "    # rename columns in preparation for merge\n",
    "    actual_df = actual_df.rename(\n",
    "        columns={\n",
    "            \"SETTLEMENTDATE\": \"forecasted_time\",\n",
    "            \"TOTALDEMAND\": \"TOTALDEMAND\",\n",
    "        }\n",
    "    )\n",
    "    # group by forecasted time and then sum demand across regions to get NEM-wide demand\n",
    "    actual_demand = (\n",
    "        actual_df.groupby(\"forecasted_time\")[\"TOTALDEMAND\"].sum().reset_index()\n",
    "    )\n",
    "\n",
    "    # calculate forecast error\n",
    "    forecast_error = calculate_p5min_forecast_demand_error(\n",
    "        actual_demand, forecast_demand\n",
    "    )\n",
    "    return forecast_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_demand_error = calculate_predispatch_demand_forecast_error_vectorised(analysis_start, analysis_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5_demand_error = calculate_p5min_demand_forecast_error_vectorised(analysis_start, analysis_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nem-data-analysis",
   "language": "python",
   "name": "nem-data-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "mystnb": {
   "execution_mode": "off"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d87edaa96a2b6f20f72be58868756e9fd14c277bec7729a5632a59d6b45b0d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
